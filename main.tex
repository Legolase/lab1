\documentclass[12pt, a4paper]{article}
\usepackage{itmo}
\usepackage{amsmath}
\newcommand\norm[1]{\lVert#1\rVert}

\begin{document}
\kafedra{Кафедра Компьютерных технологий и программирования}
\title{Лабораторная работа №1}
% \author{Пичугин~Н., гр.~3106}
\authors{Дроботов~И.B. M~3232 \\ Бандурин~В.A. M~3232 \\ Ларский~Н.А. M~3232}
%\authorfem{Процаенко~О., гр.~3106}
\prepod{Казанков~В.К.}
\maketitle

\tableofcontents
\newpage

\section{Цель}
Изучение методов оптимизации, включая градиентный спуск с постоянным шагом и метод одномерного поиска (например, метод дихотомии), а также анализ и сравнение их эффективности и сходимости на примере квадратичных функциях, необходимо выполнить следующие задачи.\\

\textbf{Постановка задачи}
\begin{enumerate}
    \item Реализуйте градиентный спуск с постоянным шагом (learning rate).
    \item Реализуйте метод одномерного поиска (метод дихотомии, метод Фибоначчи,
метод золотого сечения) и градиентный спуск на его основе.
    \item Проанализируйте траекторию градиентного спуска на примере квадратичных
функций. Для этого придумайте две-три квадратичные функции от двух пере-
менных, на которых работа методов будет отличаться.
    \item Для каждой функции:
    \begin{enumerate}
        \item исследуйте сходимость градиентного спуска с постоянным шагом, сравните
полученные результаты для выбранных функций;
        \item сравните эффективность градиентного спуска с использованием одно-
мерного поиска с точки зрения количества вычислений минимизируемой
функции и ее градиентов;
        \item исследуйте работу методов в зависимости от выбора начальной точки;
        \item исследуйте влияние нормализации (scaling) на сходимость на примере мас-
штабирования осей плохо обусловленной функции;
        \item в каждом случае нарисуйте графики с линиями уровня и траекториями
методов;
    \end{enumerate}
    \item Реализуйте генератор случайных квадратичных функций n переменных с чис-
лом обусловленности k.
    \item Исследуйте зависимость числа итераций $T (n, k)$, необходимых градиентному
спуску для сходимости в зависимости от размерности пространства $2 \leq n \leq 10^3$
и числа обусловленности оптимизируемой функции $1 \leq k \leq 10^3$ .
    \item Для получения более корректных результатов проведите множественный экс-
перимент и усредните полученные значения числа итераций.
\end{enumerate}

\section{Решение}

\begin{enumerate}
    \item Код градиентного спуска с постоянным шагом

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{image/gradient_decent_code.png}
\end{figure}

\textbf{Описание кода}

Итак, первая функция просто принимает одномерную квадратичную функцию ($f$) и считает для неё градиент в точке ($x$).

Вторая функция принимает одномерную квадратичную функцию ($f$), точку ($x$) и множитель градиета ($lr$). По данной информации даёт координаты новой точки, значение функции в которой меньше предыдущего. Другими словами это функция постоянного шага градиентного спуска.

Третья функция даёт из нескольких одномерных независимых квадратичных функций ($funcs$) n-мерную квадратичную функцию и высчитывает её значение в n-мерной точке ($pos$).

Четвёртая функция реализует градиентный спуск с постоянным шагом. Первый параметр ($funcs$) даёт набор одномерных независимых квадратичных функций, ($start$) - точка старта, ($lr$) - даёт в зависимости от ($next\_step$) либо множитель шага, либо максимальный шаг при оптимальном спуске.

    \item Код градиентного спуска с оптимизацией через метод дихотомии сечения пополам.

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{image/gradient_decent_with_dihotomy_code.png}
\end{figure}

\textbf{Описание кода}

Выглядит жутковато, но после разбора станет гораздо легче. 

Итак, первая функция представляет собой метод градиентного спуска с помощью оптимизации, код которой находится ниже. Первая часть функции нацелена на то, чтобы найти правильные параметры ($a$) и ($b$), которые уже используются самой оптимизацией. И как раз последняя строчка кода первой функции вызывает её с данными параметрами.

Вторая же функция просто вычисляет путь путём выслисления его для каждой из осей. ($contin$) отвечает за продолжение вычисления пути, пока есть хотя бы одна ось, на которой потенциально может быть новый ход. Однако, те оси на которых нашли уже минимум, блокируют повторные вызовы, через ($final$).
    

    \item 
Для анализа возьмём эти три функции:
\begin{enumerate}
    \item $f(x, y) = x^2 + y^2$ стартовая точка (100, 200)
    \item $f(x, y) = 0.01x^2 - 3x + 4 + 0.05y^2 + 4y - 15$ стартовая точка (0, 10)
    \item $f(x, y) = 0.1x^2 - 3x + \frac{1}{3}y^2 + 5y + 6$ стартовая точка (50, -30)
\end{enumerate}

\newpage

\textbf{Графики}

\begin{enumerate}
    \item 

\textbf{Постояный шаг}\\
\texttt{Шаг}: 0.1\\
\texttt{Количество ходов}: 69.\\
\texttt{Результат}: (2.05683e-05, 4.11374e-05)

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{image/3.1_gradient.png}
\end{figure}

\textbf{Метод дихотомии}\\
\texttt{Количество ходов}: 8.\\
\texttt{Результат}: (-5.55112e-15, -1.11022e-14)

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{image/3.1_gradient_opt.png}
\end{figure}

    \item 

\textbf{Постояный шаг}\\
\texttt{Шаг}: 0.1\\
\texttt{Количество ходов}: 5144.\\
\texttt{Результат}: (149.994, -39.9999)

\begin{figure}[h]
\centering
\includegraphics[width=0.63\textwidth]{image/3.2_gradient.png}
\end{figure}

\textbf{Метод дихотомии}\\
\texttt{Количество ходов}: 10.\\
\texttt{Результат}: (150, -40)

\begin{figure}[h]
\centering
\includegraphics[width=0.63\textwidth]{image/3.2_gradient_opt.png}
\end{figure}

    \item 

\textbf{Постояный шаг}\\
\texttt{Шаг}: 0.1\\
\texttt{Количество ходов}: 552.\\
\texttt{Результат}: (15.0005, -7.50751)

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{image/3.3_gradient.png}
\end{figure}

\textbf{Метод дихотомии}\\
\texttt{Количество ходов}: 22.\\
\texttt{Результат}: (15, -7.50751)

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{image/3.3_gradient_opt.png}
\end{figure}
    
\end{enumerate}

\item

\begin{enumerate}
    \item Проанализируем работы методов с постоянным шагом. В первой нункции у нас дуги возрастают с большой скоростью относительно стартовой точки. Следовательно скорость спуска будет тоже быстрой, что мы и видим. Количество шагов в этом поиске 69, что по сравнению с другими функциями и запусками поиска довольно мало.

    Если же мы посмотрим на запуск второй функции, то увидим, что дуги парабол возрастают медленнее тем самым спуск тоже оказался не маленьким. Тут нужно ещё учесть тот факт, что расстояние до точки экстремума, то есть ответа, тоже оказалось довольно больши. Поэтому количество шагов составило 5144.

    У третьей же функции дуги восрастают не так быстро как в перво и не так медленно как во второй, а расстояние до ответа примерно такое же. В результате, получаем 552 шага, которое больше чем у первой функции и гораздо меньше, чем у второй. Но стоит учесть, что это довольно приблизительный анализ, так как запуски поиска и сами функции разные.

    \item Теперь проанализируем результаты градиентного спуска с постоянным шагом. Тут реультаты немного не в таком же отношении как при использовании первого метода. Объясняется это спецификой выбора точек (a) и (b). Когда нам дают стартовую точку в одномерной квадратичной функции, то нем нужно найти противоположнуюю точку такую, чтобы произведение производных в этих точках было разным, для этого я просто беру начальный шаг и увеличиваю его в два раза пока не попадаю на противоположную сторону. В результате у нас могут получиться не всегда оптимальные значения границ.

    \item Для этого пункта немного изменим наши анализируемые функции, чтобы число обучловленности было большим и нам было бы удобнее показывать различия итераций при нормализации в будущем. Теперь наши функции такие:
    \begin{enumerate}
    \item $f(x, y) = x^2 + y^2$ стартовые точки (1, 1), (100, 200)
        \item $f(x, y) = 0.01x^2 - 3x + 4 + 0.05y^2 + 4y - 15$ стартовая точка (250, 50), (-1000, -1000)
        \item $f(x, y) = 0.5x^2 - 3x + 5y^2 + 5y + 6$ стартовая точка (-1, 1), (50, -40)
    \end{enumerate}
\newpage
    \textbf{Графики}
    \begin{enumerate}
        \item Первая функция

        \begin{enumerate}
            \item Первая стартовая точка

            \textbf{Постояный шаг}\\
            \texttt{Шаг}: 0.1\\
            \texttt{Количество ходов}: 46.\\
            \texttt{Результат}: (3.48444e-05, 3.48444e-05)
            
            \begin{figure}[h]
            \centering
            \includegraphics[width=0.55\textwidth]{image/4.3.1.1_gradient.png}
            \end{figure}

            \textbf{Дихотомия}\\
            \texttt{Количество ходов}: 4.\\
            \texttt{Результат}: (-5.55112e-17, -5.55112e-17)
            
            \begin{figure}[h]
            \centering
            \includegraphics[width=0.55\textwidth]{image/4.3.1.1_gradient_opt.png}
            \end{figure}
            
            \item Вторая стартовая точка

            \textbf{Постояный шаг}\\
            \texttt{Шаг}: 0.1\\
            \texttt{Количество ходов}: 69.\\
            \texttt{Результат}: (2.05683e-05, 4.11374e-05)
            
            \begin{figure}[h]
            \centering
            \includegraphics[width=0.6\textwidth]{image/4.3.1.2_gradient.png}
            \end{figure}

            \textbf{Дихотомия}\\
            \texttt{Количество ходов}: 8.\\
            \texttt{Результат}: (-5.55112e-15, -1.11022e-14)
            
            \begin{figure}[h]
            \centering
            \includegraphics[width=0.6\textwidth]{image/4.3.1.2_gradient_opt.png}
            \end{figure}
            
        \end{enumerate}

        \item Вторая функция

        \begin{enumerate}
            \item Первая стартовая точка
            
            \textbf{Постояный шаг}\\
            \texttt{Шаг}: 0.1\\
            \texttt{Количество ходов}: 4896.\\
            \texttt{Результат}: (150.005, -39.9999)
            
            \begin{figure}[h]
            \centering
            \includegraphics[width=0.55\textwidth]{image/4.3.2.1_gradient.png}
            \end{figure}

            \textbf{Дихотомия}\\
            \texttt{Количество ходов}: 17.\\
            \texttt{Результат}: (149.998, -40)
            
            \begin{figure}[h]
            \centering
            \includegraphics[width=0.6\textwidth]{image/4.3.2.1_gradient_opt.png}
            \end{figure}
            
            \item Вторая стартовая точка

            \textbf{Постояный шаг}\\
            \texttt{Шаг}: 0.1\\
            \texttt{Количество ходов}: 6164.\\
            \texttt{Результат}: (149.994, -40.0002)
            
            \begin{figure}[h]
            \centering
            \includegraphics[width=0.6\textwidth]{image/4.3.2.2_gradient.png}
            \end{figure}

            \textbf{Дихотомия}\\
            \texttt{Количество ходов}: 26.\\
            \texttt{Результат}: (150, -40)
            
            \begin{figure}[h]
            \centering
            \includegraphics[width=0.6\textwidth]{image/4.3.2.2_gradient_opt.png}
            \end{figure}
            
        \end{enumerate}

        \item Третья функция

        \begin{enumerate}
            \item Первая стартовая точка

            \textbf{Постояный шаг}\\
            \texttt{Шаг}: 0.1\\
            \texttt{Количество ходов}: 101.\\
            \texttt{Результат}: (2.9999, -0.5)
            
            \begin{figure}[h]
            \centering
            \includegraphics[width=0.55\textwidth]{image/4.3.3.1_gradient.png}
            \end{figure}

            \textbf{Дихотомия}\\
            \texttt{Количество ходов}: 5.\\
            \texttt{Результат}: (3, -0.5)
            
            \begin{figure}[h]
            \centering
            \includegraphics[width=0.55\textwidth]{image/4.3.3.1_gradient_opt.png}
            \end{figure}
            
            \item Вторая стартовая точка

            \textbf{Постояный шаг}\\
            \texttt{Шаг}: 0.1\\
            \texttt{Количество ходов}: 124.\\
            \texttt{Результат}: (3.0001, -0.5)
            
            \begin{figure}[h]
            \centering
            \includegraphics[width=0.6\textwidth]{image/4.3.3.2_gradient.png}
            \end{figure}

            \textbf{Дихотомия}\\
            \texttt{Количество ходов}: 10.\\
            \texttt{Результат}: (3, -0.5)
            
            \begin{figure}[h]
            \centering
            \includegraphics[width=0.55\textwidth]{image/4.3.3.2_gradient_opt.png}
            \end{figure}
            
        \end{enumerate}
    \end{enumerate}

    Проанализировав полученные результаты, можно сказать следующее. Градиентный спуск с постоянным шагом очень чувствителен к градиенту, причем если градиент меняется медленно в конце пути, то получается очень много шагов. Так что Этот метод лучше использовать с квадратичными функциями у которых ветви поднимаются быстро.

    Второй минус обычного градиента заключается в том, что если число обусловленности многомерной вадратичной функции большое, то путь обычного градиента делает большую дугу, что сказывается на большем количестве ходов по правилу треугольника.

    Третий не существенный минус заключается в не такой хорошей точности решения как у градиента с дихотомией. Так как обычному градиенту в конце нужно всё больше и больше ходов для удовлетворительной точности, тогда как градиент с оптимизацией не ограничен стороной стартовой точки от точки решения и поэтому получаются более точные результаты.

    Ещё стоит отметить скорость увеличения ходов градиентного спуска с оптимизацией от дальности стартовой точки до ответа. Она логарифмическая. Оно и понятно, так как оптимизация похожа на бинарный поиск, который имеет ту же ассимптотику.

    \item Для этого пункта возьмём как раз третью функцию из предыдущего пункта $f(x, y) = 0.5x^2 - 3x + 5y^2 + 5y + 6$. У неё $k$ - коеффициент обусловленности равен 10. Посмотрим как выглядел обычный градиентный спуск от дальней точки 

    \textbf{Постояный шаг}\\
    \texttt{Шаг}: 0.1\\
    \texttt{Количество ходов}: 124.\\
    \texttt{Результат}: (3.0001, -0.5)
    
    \begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{image/4.3.3.2_gradient.png}
    \end{figure}

    И заменим коеффициент при $x^2$ на 2. 
    
    Получим вот такую функцию $f(x, y) = 2x^2 - 3x + 5y^2 + 5y + 6$.

    \textbf{Постояный шаг}\\
    \texttt{Шаг}: 0.1\\
    \texttt{Количество ходов}: 29.\\
    \texttt{Результат}: (3, -0.5)
    
    \begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{image/4.4.1_gradient.png}
    \end{figure}

    Как мы видим, количество ходов уменьшилось почти в 5 раз при изменении коеффициента обусловленности в 4 раза, что довольно любопытно. То есть тут можно заметить линейную зависимость между этими двумя показателями.
    
\end{enumerate}

\item Реализация n-мерной квадратичной функции с k-обусловленностью.

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{image/5_code.png}
\end{figure}

\item Цветовая карта зависимости числа итераций $T(n,k)$

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{image/6_iterations.png}
\end{figure}

В результате проведенного эксперимента было выяснено, что размерность пространства не оказывает значительного влияния на результат работы алгоритма, важным фактором является лишь число обусловленности функции. При использовании малого шага и низком числе обусловленности алгоритму требуется большое количество итераций. В случае, если число обусловленности высокое, алгоритму потребуется меньшее количество шагов, так как он перескакивает через минимум функции, и процесс оптимизации завершается.

Под конец добавлю график отношения количества итераций у методов к расстоянию стартовой точки до ответа.

\newpage

\begin{enumerate}
    \item $f(x, y) = x^2 + y^2$
    \begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{image/difference.png}
    \end{figure}
    
    \item $f(x, y) = 0.1x^2 + 0.1y^2$
    \begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{image/difference-2.png}
    \end{figure}
\end{enumerate}

Можно заметить, что чем меньше параметр (a) у квадратичной функции, то различия в количистве ходов для каждого метода будут только усиливаться.
\end{enumerate}

\section{Вывод}
В процессе работы были изучены методы оптимизации, включая градиентный спуск и градиентный спуск на основе дихотомии. Было проведено сравнение эффективности этих методов на различных функциях, выполнены поставленные задачи и произведен анализ перечисленных методов.

\end{document}
